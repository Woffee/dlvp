"""
Functions 2 vectors. The functions are generated by Scitools Understand.

@Time    : 7/18/21
@Author  : Wenbo
"""


import pandas as pd
import argparse
import json
import os
import tempfile
import subprocess
import time
import logging
import hashlib
import preprocess_code
import longpath
from pathlib import Path

BASE_DIR = os.path.dirname(os.path.abspath(__file__))

joern_path = "/opt/joern/"

# make dirs
# folders = ['data/', 'logs/', 'projects/', 'data/function2vec3', "data/preprocess", 'data/function2vec3/models']
# for f in folders:
#     Path(f).mkdir(parents=True, exist_ok=True)

# args
parser = argparse.ArgumentParser(description='Test for argparse')
parser.add_argument('--cur_p', help='the No. of current progress', type=int, default=9)
parser.add_argument('--tasks_file', help='tasks_file', type=str, default='cflow/tasks.json')
parser.add_argument('--functions_path', help='functions_path', type=str, default='/data/function2vec2/functions_jy')
parser.add_argument('--save_path', help='functions_path', type=str, default = BASE_DIR + "/data/function2vec2")

# parser.add_argument('--all_func_trees_json_file', help='all_func_trees_json_file', type=str,
#                     default= SAVE_PATH + '/all_functions_with_trees.json')
# parser.add_argument('--all_func_trees_file', help='all_func_trees_file', type=str,
#                     default= SAVE_PATH + '/all_functions_with_trees.csv')
# parser.add_argument('--all_func_embedding_file', help='all_func_embedding_file', type=str, default= SAVE_PATH + '/all_func_embedding_ref.csv')

args = parser.parse_args()

SAVE_PATH = args.save_path
Path(SAVE_PATH).mkdir(parents=True, exist_ok=True)

# log file
Path("logs").mkdir(parents=True, exist_ok=True)
now_time = time.strftime("%Y-%m-%d_%H-%M", time.localtime())
logging.basicConfig(level=logging.INFO,
                    format='%(asctime)s %(levelname)s %(filename)s line: %(lineno)s - %(message)s',
                    datefmt='%Y-%m-%d %H:%M:%S',
                    filename=BASE_DIR + '/logs/' + now_time + "_p%d.log" % int(args.cur_p))
logger = logging.getLogger(__name__)
logger.info("function2vec parameters %s", args)

def findAllFile(base):
    for root, ds, fs in os.walk(base):
        for f in fs:
            fullname = os.path.join(root, f)
            yield fullname

def generate_prolog(code):
    if code.strip() == "":
        return ""
    tmp_dir = tempfile.TemporaryDirectory()
    md5_v = hashlib.md5(code.encode()).hexdigest()
    short_filename = "func_" + md5_v + ".cpp"
    with open(tmp_dir.name + "/" + short_filename, 'w') as f:
        f.write(code)
    # print(short_filename)
    logger.info(short_filename)
    subprocess.check_call(["/opt/joern/joern-parse", tmp_dir.name, "--out", tmp_dir.name + "/cpg.bin.zip"])

    tree = subprocess.check_output(
        "cd /opt/joern && ./joern --script joern_cfg_to_dot.sc --params cpgFile=" + tmp_dir.name + "/cpg.bin.zip",
        shell=True,
        universal_newlines=True,
    )
    pos = tree.find("digraph g {")
    print(pos)
    if pos > 0:
        tree = tree[pos:]
    tmp_dir.cleanup()
    return tree

def read_functions(repo_name, entities_filename, non_vul_commits):
    """
    读取一个 entities 文件里的所有 functions。

    """
    functions = []
    cve_id = entities_filename.split("/")[-1].replace("-entities.json", "")

    # read entities
    json_str = ""
    with open(entities_filename) as f:
        json_str = f.read().strip()
    if json_str == "":
        return []
    cve_functions = json.loads(json_str)

    for k, v in cve_functions.items():
        vul = 0 if v['commit'] in non_vul_commits else 1

        functions.append({
            'repo_name': repo_name,
            'cve_id': cve_id,
            'commit': v['commit'],
            'func_key': k,
            'name': v['name'],
            'uniquename': v['uniquename'],
            'contents': v['contents'],
            'kind': v['kind'],
            'type': v['type'],
            'vul': vul,
        })

    return functions

# 遍历所有已经生成 trees 的 function，返回 func_key 列表。
def get_existed_trees_func_id(filename):
    res = []
    if os.path.exists(filename):
        with open(filename, "r") as f:
            for line in f.readlines():
                l = line.strip()
                if l=="":
                    continue
                item = json.loads(l)
                if item['func_key'] not in res:
                    res.append(item['func_key'])
    return res

# for jiahao's data
def get_func(repo_name, cve_id, commit, vul, level, func_key, code, tree):
    return {
        'repo_name': repo_name,
        'cve_id': cve_id,
        'commit': commit,
        'func_key': func_key,
        'name': '',
        'uniquename': func_key,
        'vul': vul,
        'level': level, # 0: current_function, 1: callee, 2: caller
        'contents': code,
        'kind': '',
        'type': '',
        'tree': tree,
    }


def jh_data_to_json(jh_data_file, to_file_entities, to_file_relations):
    logger.info("running jh_data_to_json...")
    df = pd.read_csv(jh_data_file)
    df = df.fillna("")

    total = len(df)

    for index, row in df.iterrows():
        if index % 1000 == 0:
            logger.info("now: {} / {}".format(index, total))

        func_key = "{}_{}_{}_{}".format(row['project'], row['CVE ID'], row['commit_id'], index)
        print(func_key)
        func = get_func(row['project'], row['CVE ID'], row['commit_id'], row['vul'], 0, func_key, row['code'],
                        row['trees'])
        with open(to_file_entities, "a") as fw:
            fw.write(json.dumps(func) + "\n")

        relations = []
        if row['code_callee'] != "":
            code_callees = row['code_callee'].split("-" * 50)
            tree_callees = row['trees_callee'].split("-" * 50)
            for ii, code in enumerate(code_callees):
                if ii >= len(tree_callees):
                    break

                sub_func_key = func_key + "_callee_" + str(ii)
                tree = tree_callees[ii]
                sub_func = get_func(row['project'], row['CVE ID'], row['commit_id'], row['vul'], 1, sub_func_key, code,
                                    tree)
                with open(to_file_entities, "a") as fw:
                    fw.write(json.dumps(sub_func) + "\n")

                relations.append({
                    "type": "call",
                    "value": sub_func_key,
                })

        if row['code_caller'] != "":
            code_callees = row['code_caller'].split("-" * 50)
            tree_callees = row['trees_caller'].split("-" * 50)
            for ii, code in enumerate(code_callees):
                if ii >= len(tree_callees):
                    break

                sub_func_key = func_key + "_caller_" + str(ii)
                tree = tree_callees[ii]
                sub_func = get_func(row['project'], row['CVE ID'], row['commit_id'], row['vul'], -1, sub_func_key, code,
                                    tree)
                with open(to_file_entities, "a") as fw:
                    fw.write(json.dumps(sub_func) + "\n")

                relations.append({
                    "type": "callby",
                    "value": sub_func_key,
                })

        relation_json = {
            func_key: relations
        }
        with open(to_file_relations, "a") as fw:
            fw.write(json.dumps(relation_json) + "\n")
    print("done")
    logger.info("saved to: {}".format(to_file_entities))
    logger.info("saved to: {}".format(to_file_relations))

if __name__ == '__main__':
    tasks_file = args.tasks_file
    functions_path = args.functions_path

    # 先每个 function 对应一个 json object，追加保存到到 all_func_trees_json_file，再合并到 all_func_trees_file。
    all_func_trees_json_file = SAVE_PATH + '/all_functions_with_trees.json'
    all_func_trees_json_file_merged = SAVE_PATH + '/all_functions_with_trees_merged_jh.json'
    all_func_trees_file = SAVE_PATH + '/all_functions_with_trees.csv'

    cur_p = int(args.cur_p)


    # 由于 Linux 还没有全部找完 caller callee，先把现有的生成到一个单独文件里
    # linux_func_trees_json_file = SAVE_PATH + "/all_functions_with_trees_linux.json"
    # if not os.path.exists(linux_func_trees_json_file):
    #     cc = 0
    #     with open(tasks_file, 'r') as taskFile:
    #         taskDesc = json.load(taskFile)
    #
    #         for repoName in taskDesc:
    #             if repoName != 'linux':
    #                 continue
    #             logger.info("now project: %s" % repoName)
    #             project_path = os.path.join(functions_path, repoName)
    #
    #             for cve_id, non_vul_commits in taskDesc[repoName]['vuln'].items():
    #                 logger.info("now cve_id: %s" % cve_id)
    #
    #                 entities_file = "%s/%s-entities.json" % (project_path, cve_id)
    #                 cve_functions = read_functions(repoName, entities_file, non_vul_commits)
    #
    #                 for func in cve_functions:
    #                     func['tree'] = generate_prolog(func['contents'])
    #                     with open(linux_func_trees_json_file, "a") as fw:
    #                         fw.write(json.dumps(func) + "\n")
    #                         logger.info("saved: %s" % func['func_key'])
    #                     cc += 1
    #                     if cc % 1000 == 0:
    #                         cmd = "rm /tmp/joern-default*"
    #                         p = os.popen(cmd)
    #                         x = p.read()
    #     logger.info("sub progress %d done" % cur_p)
    # exit()

    # 把 jiahao 的数据集转换成 jy 的格式。
    jh_data_file = "/data/jiahao_data/train_10425_val_3459_test_3452_im_test_9_15780_im_test_16_26826.csv"
    jh_to_file_entitties = "/data/jiahao_data/jh_entities.json"
    jh_to_file_relations = "/data/jiahao_data/jh_relations.json"
    # jh_data_to_json(jh_data_file, jh_to_file_entitties, jh_to_file_relations)
    # exit()

    # 把 jiahao 的数据也 merge 到一起（一次性代码）
    # logger.info("merge jiahao's data")
    # with open(jh_to_file_entitties, "r") as f:
    #     for line in f.readlines():
    #         with open(all_func_trees_json_file_merged, "a") as fw:
    #             fw.write(line.strip() + "\n")
    # logger.info("saved to: {}".format(all_func_trees_json_file_merged))
    # exit()

    if not os.path.exists(all_func_trees_json_file_merged):
        if cur_p < 9: # 用 9 来代替是否合并处理好的数据
            # get trees from code using Joern
            all_func_trees_json_file_p = SAVE_PATH + "/all_functions_with_trees_%d.json" % cur_p
            existed = get_existed_trees_func_id(all_func_trees_json_file_p)
            cc = 0

            with open(tasks_file, 'r') as taskFile:
                taskDesc = json.load(taskFile)

                # 数据比较耗时间，分成 4 个来同时跑，每个跑 60 个 projects
                batch_p = 60
                finished_repos = ['accountsservice','ghostpdl','aircrack-ng','atheme','axtls-8266','barebox','beanstalkd','bfgminer','bitlbee','bubblewrap','busybox','bzrtp','capstone','cherokee','cJSON','collectd','htcondor','conntrack-tools','corosync']
                all_repo_names = list(taskDesc.keys())
                all_repo_names = list(set(all_repo_names) - set(finished_repos))

                cur_batch = all_repo_names[batch_p * cur_p: batch_p * (cur_p + 1)]
                print("now cur_p: %d" % cur_p)
                print("now len of cur_batch: %d" % len(cur_batch))
                logger.info("now cur_p: %d" % cur_p)
                logger.info("now len of cur_batch: %d" % len(cur_batch))


                for repoName in cur_batch:
                    if repoName == 'linux':
                        continue
                    logger.info("now project: %s" % repoName)
                    project_path = os.path.join(functions_path, repoName)

                    for cve_id, non_vul_commits in taskDesc[repoName]['vuln'].items():
                        logger.info("now cve_id: %s" % cve_id)

                        entities_file = "%s/%s-entities.json" % (project_path, cve_id)
                        cve_functions = read_functions(repoName, entities_file, non_vul_commits)

                        for func in cve_functions:
                            if func['func_key'] not in existed:
                                func['tree'] = generate_prolog(func['contents'])

                                with open(all_func_trees_json_file_p, "a") as fw:
                                    fw.write(json.dumps(func) + "\n")
                                    logger.info("saved: %s" % func['func_key'])
                                cc += 1
                                if cc % 1000 == 0:
                                    cmd = "rm /tmp/joern-default*"
                                    p = os.popen(cmd)
                                    x = p.read()
            logger.info("sub progress %d done" % cur_p)
            exit()
        else:
            logger.info("merge functions together (json)")
            # save jsons (with trees) to a csv file
            # all_funcs = []
            all_funcs_keys = []
            with open(all_func_trees_json_file, "r") as f:
                for line in f.readlines():
                    l = line.strip()
                    if l == "":
                        continue
                    func = json.loads(l)
                    if func['func_key'] not in all_funcs_keys:
                        # all_funcs.append(func)
                        all_funcs_keys.append(func['func_key'])
                        with open(all_func_trees_json_file_merged, "a") as fw:
                            fw.write(l + "\n")
            for pp in range(4):
                all_func_trees_json_file_p = SAVE_PATH + "/all_functions_with_trees_%d.json" % pp
                if not os.path.exists(all_func_trees_json_file_p):
                    continue
                print("reading:", all_func_trees_json_file_p)
                logger.info("reading: %s" % all_func_trees_json_file_p )
                with open(all_func_trees_json_file_p, "r") as f:
                    for line in f.readlines():
                        l = line.strip()
                        if l == "":
                            continue

                        func = json.loads(l)
                        if func['func_key'] not in all_funcs_keys:
                            # all_funcs.append(func)
                            all_funcs_keys.append(func['func_key'])
                            with open(all_func_trees_json_file_merged, "a") as fw:
                                fw.write(l + "\n")
            logger.info("merged: " + all_func_trees_json_file_merged)
            # df_functions = pd.DataFrame(all_funcs)
            # df_functions.to_csv(all_func_trees_file, sep=',', index=None)
            # print("saved to:", all_func_trees_file)
            # logger.info("saved to: %s" % all_func_trees_file)

    # preprocess code
    # REF
    input_file = all_func_trees_json_file_merged
    to_embedding_file_ref = SAVE_PATH + "/all_func_embedding_ref.csv"
    if not os.path.exists(to_embedding_file_ref):
        print("running REF")
        logger.info("running REF")
        tmp_directory = tempfile.TemporaryDirectory()
        graph2vec_input_dir = preprocess_code.preprocess_all_joern_for_graph2vec(input_file, tmp_directory.name, "REF",
                                                                                 "CALL", num_partitions=10)
        output_file = to_embedding_file_ref
        preprocess_code.run_graph2vec(graph2vec_input_dir, output_file, num_graph2vec_workers=2, num_epoch=10)
        logger.info("running REF done")

    # DEF
    to_embedding_file_def = SAVE_PATH + "/all_func_embedding_def.csv"
    if not os.path.exists(to_embedding_file_def):
        print("running DEF")
        logger.info("running DEF")
        tmp_directory = tempfile.TemporaryDirectory()
        graph2vec_input_dir = preprocess_code.preprocess_all_joern_for_graph2vec(input_file, tmp_directory.name, "REACHING_DEF",
                                                                                 "EVAL_TYPE", num_partitions=10)
        output_file = to_embedding_file_def
        preprocess_code.run_graph2vec(graph2vec_input_dir, output_file, num_graph2vec_workers=2, num_epoch=10)
        logger.info("running DEF done")

    # PDT
    to_embedding_file_pdt = SAVE_PATH + "/all_func_embedding_pdt.csv"
    if not os.path.exists(to_embedding_file_pdt):
        print("running PDT")
        logger.info("running PDT")
        tmp_directory = tempfile.TemporaryDirectory()
        graph2vec_input_dir = preprocess_code.preprocess_all_joern_for_graph2vec(input_file, tmp_directory.name,
                                                                                 "CFG","REF", num_partitions=10, PDT=True)
        output_file = to_embedding_file_pdt
        preprocess_code.run_graph2vec(graph2vec_input_dir, output_file, num_graph2vec_workers=2, num_epoch=10)
        logger.info("running PDT done")


    input_file = all_func_trees_json_file_merged
    # LP
    Path(SAVE_PATH+"/models").mkdir(parents=True, exist_ok=True)
    to_embedding_file_lp = SAVE_PATH + "/all_func_embedding_lp.pkl"
    if not os.path.exists(to_embedding_file_lp + ".combine"):
        print("running LP")
        logger.info("running LP")
        tmp_directory = tempfile.TemporaryDirectory()
        all_functions_with_lp_file = SAVE_PATH + '/all_functions_with_trees_lp.json'
        if not os.path.exists(all_functions_with_lp_file):
            longpath.preprocess_longpath(input_file, all_functions_with_lp_file, "json")

        logger.info("run_longpath()...")
        output_file = to_embedding_file_lp
        w2v_lp_model_file_combine = SAVE_PATH + "/models/w2v_lp_combine.bin"
        w2v_lp_model_file_greedy = SAVE_PATH + "/models/w2v_lp_greedy.bin"
        longpath.run_longpath(all_functions_with_lp_file, output_file, w2v_lp_model_file_combine, w2v_lp_model_file_greedy)
        logger.info("running LP done")

    # NS
    to_embedding_file_ns = SAVE_PATH + "/all_func_embedding_ns.pkl"
    if not os.path.exists(to_embedding_file_ns):
        print("running NS")
        logger.info("running NS")
        output_file = to_embedding_file_ns
        w2v_ns_model = SAVE_PATH + "/models/w2v_ns.bin"
        longpath.run_ns(input_file, output_file, w2v_ns_model)
        logger.info("running NS done")

    print("done")
    logger.info("done")