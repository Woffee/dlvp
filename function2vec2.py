"""
Functions 2 vectors. The functions are generated by Scitools Understand.

@Time    : 7/18/21
@Author  : Wenbo
"""


import pandas as pd
import argparse
import json
import os
import tempfile
import subprocess
import time
import logging
import hashlib
import preprocess_code
import longpath
from pathlib import Path

BASE_DIR = os.path.dirname(os.path.abspath(__file__))

joern_path = "/opt/joern/"

# make dirs
folders = ['data/', 'logs/', 'projects/', 'data/function2vec3', "data/preprocess", 'data/function2vec3/models']
for f in folders:
    Path(f).mkdir(parents=True, exist_ok=True)

# args
parser = argparse.ArgumentParser(description='Test for argparse')
parser.add_argument('--cur_p', help='the No. of current progress', type=int, default=9)
parser.add_argument('--tasks_file', help='tasks_file', type=str, default='cflow/tasks.json')
parser.add_argument('--functions_path', help='functions_path', type=str, default='data/function2vec2/functions_jy')
parser.add_argument('--save_path', help='functions_path', type=str, default = BASE_DIR + "/data/function2vec2")

# parser.add_argument('--all_func_trees_json_file', help='all_func_trees_json_file', type=str,
#                     default= SAVE_PATH + '/all_functions_with_trees.json')
# parser.add_argument('--all_func_trees_file', help='all_func_trees_file', type=str,
#                     default= SAVE_PATH + '/all_functions_with_trees.csv')
# parser.add_argument('--all_func_embedding_file', help='all_func_embedding_file', type=str, default= SAVE_PATH + '/all_func_embedding_ref.csv')

args = parser.parse_args()

SAVE_PATH = args.save_path


# log file
now_time = time.strftime("%Y-%m-%d_%H-%M", time.localtime())
logging.basicConfig(level=logging.INFO,
                    format='%(asctime)s %(levelname)s %(filename)s line: %(lineno)s - %(message)s',
                    datefmt='%Y-%m-%d %H:%M:%S',
                    filename=BASE_DIR + '/logs/' + now_time + "_p%d.log" % int(args.cur_p))
logger = logging.getLogger(__name__)
logger.info("function2vec parameters %s", args)

def findAllFile(base):
    for root, ds, fs in os.walk(base):
        for f in fs:
            fullname = os.path.join(root, f)
            yield fullname

def generate_prolog(code):
    if code.strip() == "":
        return ""
    tmp_dir = tempfile.TemporaryDirectory()
    md5_v = hashlib.md5(code.encode()).hexdigest()
    short_filename = "func_" + md5_v + ".cpp"
    with open(tmp_dir.name + "/" + short_filename, 'w') as f:
        f.write(code)
    # print(short_filename)
    logger.info(short_filename)
    subprocess.check_call(["/opt/joern/joern-parse", tmp_dir.name, "--out", tmp_dir.name + "/cpg.bin.zip"])

    tree = subprocess.check_output(
        "cd /opt/joern && ./joern --script joern_cfg_to_dot.sc --params cpgFile=" + tmp_dir.name + "/cpg.bin.zip",
        shell=True,
        universal_newlines=True,
    )
    pos = tree.find("digraph g {")
    print(pos)
    if pos > 0:
        tree = tree[pos:]
    tmp_dir.cleanup()
    return tree

def read_functions(repo_name, entities_filename, non_vul_commits):
    """
    读取一个 entities 文件里的所有 functions。

    """
    functions = []
    cve_id = entities_filename.split("/")[-1].replace("-entities.json", "")

    # read entities
    json_str = ""
    with open(entities_filename) as f:
        json_str = f.read().strip()
    if json_str == "":
        return []
    cve_functions = json.loads(json_str)

    for k, v in cve_functions.items():
        vul = 0 if v['commit'] in non_vul_commits else 1

        functions.append({
            'repo_name': repo_name,
            'cve_id': cve_id,
            'commit': v['commit'],
            'func_key': k,
            'name': v['name'],
            'uniquename': v['uniquename'],
            'contents': v['contents'],
            'kind': v['kind'],
            'type': v['type'],
            'vul': vul,
        })

    return functions

# 遍历所有已经生成 trees 的 function，返回 func_key 列表。
def get_existed_trees_func_id(filename):
    res = []
    if os.path.exists(filename):
        with open(filename, "r") as f:
            for line in f.readlines():
                l = line.strip()
                if l=="":
                    continue
                item = json.loads(l)
                if item['func_key'] not in res:
                    res.append(item['func_key'])
    return res


if __name__ == '__main__':
    tasks_file = args.tasks_file
    functions_path = args.functions_path

    # 先每个 function 对应一个 json object，追加保存到到 all_func_trees_json_file，再合并到 all_func_trees_file。
    all_func_trees_json_file = SAVE_PATH + '/all_functions_with_trees.json'
    all_func_trees_json_file_merged = SAVE_PATH + '/all_functions_with_trees_merged.json'
    all_func_trees_file = SAVE_PATH + '/all_functions_with_trees.csv'

    cur_p = int(args.cur_p)

    SUB_SAVEPATH = BASE_DIR + "/data/function2vec2_%d" % cur_p
    Path(SUB_SAVEPATH).mkdir(parents=True, exist_ok=True)

    if not os.path.exists(all_func_trees_json_file_merged):
        if cur_p < 9: # 用 9 来代替是否合并处理好的数据
            # get trees from code using Joern
            all_func_trees_json_file_p = SUB_SAVEPATH + '/all_functions_with_trees.json'
            existed = get_existed_trees_func_id(all_func_trees_json_file_p)
            cc = 0

            with open(tasks_file, 'r') as taskFile:
                taskDesc = json.load(taskFile)

                # 数据比较耗时间，分成 4 个来同时跑，每个跑 60 个 projects
                batch_p = 60
                finished_repos = ['accountsservice','ghostpdl','aircrack-ng','atheme','axtls-8266','barebox','beanstalkd','bfgminer','bitlbee','bubblewrap','busybox','bzrtp','capstone','cherokee','cJSON','collectd','htcondor','conntrack-tools','corosync']
                all_repo_names = list(taskDesc.keys())
                all_repo_names = list(set(all_repo_names) - set(finished_repos))

                cur_batch = all_repo_names[batch_p * cur_p: batch_p * (cur_p + 1)]
                print("now cur_p: %d" % cur_p)
                print("now len of cur_batch: %d" % len(cur_batch))
                logger.info("now cur_p: %d" % cur_p)
                logger.info("now len of cur_batch: %d" % len(cur_batch))


                for repoName in cur_batch:
                    if repoName == 'linux':
                        continue
                    logger.info("now project: %s" % repoName)
                    project_path = os.path.join(functions_path, repoName)

                    for cve_id, non_vul_commits in taskDesc[repoName]['vuln'].items():
                        logger.info("now cve_id: %s" % cve_id)

                        entities_file = "%s/%s-entities.json" % (project_path, cve_id)
                        cve_functions = read_functions(repoName, entities_file, non_vul_commits)

                        for func in cve_functions:
                            if func['func_key'] not in existed:
                                func['tree'] = generate_prolog(func['contents'])

                                with open(all_func_trees_json_file_p, "a") as fw:
                                    fw.write(json.dumps(func) + "\n")
                                    logger.info("saved: %s" % func['func_key'])
                                cc += 1
                                if cc % 1000 == 0:
                                    cmd = "rm /tmp/joern-default*"
                                    p = os.popen(cmd)
                                    x = p.read()
            logger.info("sub progress %d done" % cur_p)
            exit()
        else:
            logger.info("merge functions together (json)")
            # save jsons (with trees) to a csv file
            # all_funcs = []
            all_funcs_keys = []
            with open(all_func_trees_json_file, "r") as f:
                for line in f.readlines():
                    l = line.strip()
                    if l == "":
                        continue
                    func = json.loads(l)
                    if func['func_key'] not in all_funcs_keys:
                        # all_funcs.append(func)
                        all_funcs_keys.append(func['func_key'])
                        with open(all_func_trees_json_file_merged, "a") as fw:
                            fw.write(l + "\n")
            for pp in range(4):
                all_func_trees_json_file_p = SAVE_PATH + "_%d/all_functions_with_trees.json" % pp
                if not os.path.exists(all_func_trees_json_file_p):
                    continue
                print("reading:", all_func_trees_json_file_p)
                with open(all_func_trees_json_file_p, "r") as f:
                    for line in f.readlines():
                        l = line.strip()
                        if l == "":
                            continue

                        func = json.loads(l)
                        if func['func_key'] not in all_funcs_keys:
                            # all_funcs.append(func)
                            all_funcs_keys.append(func['func_key'])
                            with open(all_func_trees_json_file_merged, "a") as fw:
                                fw.write(l + "\n")
            logger.info("merged: " + all_func_trees_json_file_merged)
            # df_functions = pd.DataFrame(all_funcs)
            # df_functions.to_csv(all_func_trees_file, sep=',', index=None)
            # print("saved to:", all_func_trees_file)
            # logger.info("saved to: %s" % all_func_trees_file)

    # preprocess code
    # REF
    input_file = all_func_trees_json_file_merged
    to_embedding_file_ref = SAVE_PATH + "/all_func_embedding_ref.csv"
    if not os.path.exists(to_embedding_file_ref):
        print("running REF")
        logger.info("running REF")
        tmp_directory = tempfile.TemporaryDirectory()
        graph2vec_input_dir = preprocess_code.preprocess_all_joern_for_graph2vec(input_file, tmp_directory.name, "REF",
                                                                                 "CALL", num_partitions=10)
        output_file = to_embedding_file_ref
        preprocess_code.run_graph2vec(graph2vec_input_dir, output_file, num_graph2vec_workers=2, num_epoch=10)
        logger.info("running REF done")

    # DEF
    to_embedding_file_def = SAVE_PATH + "/all_func_embedding_def.csv"
    if not os.path.exists(to_embedding_file_def):
        print("running DEF")
        logger.info("running DEF")
        tmp_directory = tempfile.TemporaryDirectory()
        graph2vec_input_dir = preprocess_code.preprocess_all_joern_for_graph2vec(input_file, tmp_directory.name, "REACHING_DEF",
                                                                                 "EVAL_TYPE", num_partitions=10)
        output_file = to_embedding_file_def
        preprocess_code.run_graph2vec(graph2vec_input_dir, output_file, num_graph2vec_workers=2, num_epoch=10)
        logger.info("running DEF done")

    # PDT
    to_embedding_file_pdt = SAVE_PATH + "/all_func_embedding_pdt.csv"
    if not os.path.exists(to_embedding_file_pdt):
        print("running PDT")
        logger.info("running PDT")
        tmp_directory = tempfile.TemporaryDirectory()
        graph2vec_input_dir = preprocess_code.preprocess_all_joern_for_graph2vec(input_file, tmp_directory.name,
                                                                                 "CFG","REF", num_partitions=10, PDT=True)
        output_file = to_embedding_file_pdt
        preprocess_code.run_graph2vec(graph2vec_input_dir, output_file, num_graph2vec_workers=2, num_epoch=10)
        logger.info("running PDT done")


    input_file = all_func_trees_json_file_merged
    # LP
    to_embedding_file_lp = SAVE_PATH + "/all_func_embedding_lp.pkl"
    if not os.path.exists(to_embedding_file_lp + ".combine"):
        print("running LP")
        logger.info("running LP")
        tmp_directory = tempfile.TemporaryDirectory()
        all_functions_with_lp_file = SAVE_PATH + '/all_functions_with_trees_lp.json'
        if not os.path.exists(all_functions_with_lp_file):
            longpath.preprocess_longpath(input_file, all_functions_with_lp_file, "json")

        logger.info("run_longpath()...")
        output_file = to_embedding_file_lp
        w2v_lp_model_file_combine = SAVE_PATH + "/models/w2v_lp_combine.bin"
        w2v_lp_model_file_greedy = SAVE_PATH + "/models/w2v_lp_greedy.bin"
        longpath.run_longpath(all_functions_with_lp_file, output_file, w2v_lp_model_file_combine, w2v_lp_model_file_greedy)
        logger.info("running LP done")

    # NS
    to_embedding_file_ns = SAVE_PATH + "/all_func_embedding_ns.pkl"
    if not os.path.exists(to_embedding_file_ns):
        print("running NS")
        logger.info("running NS")
        output_file = to_embedding_file_ns
        w2v_ns_model = SAVE_PATH + "/models/w2v_ns.bin"
        longpath.run_ns(input_file, output_file, w2v_ns_model)
        logger.info("running NS done")

    print("done")
    logger.info("done")